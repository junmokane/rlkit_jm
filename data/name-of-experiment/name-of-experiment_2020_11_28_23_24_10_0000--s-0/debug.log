2020-11-28 23:25:29.641833 KST | [name-of-experiment_2020_11_28_23_24_10_0000--s-0] Epoch 0 finished
----------------------------------------------  --------------
replay_buffer/size                              2000
trainer/num train calls                         1000
trainer/QF1 Loss                                  13.5347
trainer/QF2 Loss                                  13.5025
trainer/Policy Loss                               -3.97605
trainer/Q1 Predictions Mean                       -0.00834318
trainer/Q1 Predictions Std                         0.00485879
trainer/Q1 Predictions Max                         0.00281431
trainer/Q1 Predictions Min                        -0.0246794
trainer/Q2 Predictions Mean                       -0.00376042
trainer/Q2 Predictions Std                         0.0058827
trainer/Q2 Predictions Max                         0.00952145
trainer/Q2 Predictions Min                        -0.0236481
trainer/Q Targets Mean                             3.57634
trainer/Q Targets Std                              0.827686
trainer/Q Targets Max                              5.95426
trainer/Q Targets Min                              0.759551
trainer/Log Pis Mean                              -3.98514
trainer/Log Pis Std                                0.533899
trainer/Log Pis Max                               -2.50582
trainer/Log Pis Min                               -5.22533
trainer/policy/mean Mean                          -6.25752e-05
trainer/policy/mean Std                            0.00151846
trainer/policy/mean Max                            0.00531059
trainer/policy/mean Min                           -0.00638671
trainer/policy/normal/std Mean                     0.999119
trainer/policy/normal/std Std                      0.00145937
trainer/policy/normal/std Max                      1.00485
trainer/policy/normal/std Min                      0.993797
trainer/policy/normal/log_std Mean                -0.000882696
trainer/policy/normal/log_std Std                  0.00146087
trainer/policy/normal/log_std Max                  0.00483937
trainer/policy/normal/log_std Min                 -0.00622225
trainer/Alpha                                      1
trainer/Alpha Loss                                -0
exploration/num steps total                     2000
exploration/num paths total                        2
exploration/path length Mean                    1000
exploration/path length Std                        0
exploration/path length Max                     1000
exploration/path length Min                     1000
exploration/Rewards Mean                          -0.3072
exploration/Rewards Std                            0.627124
exploration/Rewards Max                            1.6691
exploration/Rewards Min                           -2.60757
exploration/Returns Mean                        -307.2
exploration/Returns Std                            0
exploration/Returns Max                         -307.2
exploration/Returns Min                         -307.2
exploration/Actions Mean                          -0.00692176
exploration/Actions Std                            0.631997
exploration/Actions Max                            0.997723
exploration/Actions Min                           -0.998527
exploration/Num Paths                              1
exploration/Average Returns                     -307.2
exploration/env_infos/final/reward_run Mean        0.236999
exploration/env_infos/final/reward_run Std         0
exploration/env_infos/final/reward_run Max         0.236999
exploration/env_infos/final/reward_run Min         0.236999
exploration/env_infos/initial/reward_run Mean     -0.164998
exploration/env_infos/initial/reward_run Std       0
exploration/env_infos/initial/reward_run Max      -0.164998
exploration/env_infos/initial/reward_run Min      -0.164998
exploration/env_infos/reward_run Mean             -0.0675185
exploration/env_infos/reward_run Std               0.621846
exploration/env_infos/reward_run Max               1.94035
exploration/env_infos/reward_run Min              -2.26949
exploration/env_infos/final/reward_ctrl Mean      -0.157079
exploration/env_infos/final/reward_ctrl Std        0
exploration/env_infos/final/reward_ctrl Max       -0.157079
exploration/env_infos/final/reward_ctrl Min       -0.157079
exploration/env_infos/initial/reward_ctrl Mean    -0.331164
exploration/env_infos/initial/reward_ctrl Std      0
exploration/env_infos/initial/reward_ctrl Max     -0.331164
exploration/env_infos/initial/reward_ctrl Min     -0.331164
exploration/env_infos/reward_ctrl Mean            -0.239681
exploration/env_infos/reward_ctrl Std              0.0738423
exploration/env_infos/reward_ctrl Max             -0.0258478
exploration/env_infos/reward_ctrl Min             -0.476492
evaluation/num steps total                      5000
evaluation/num paths total                         5
evaluation/path length Mean                     1000
evaluation/path length Std                         0
evaluation/path length Max                      1000
evaluation/path length Min                      1000
evaluation/Rewards Mean                           -0.000243855
evaluation/Rewards Std                             0.0126893
evaluation/Rewards Max                             0.200411
evaluation/Rewards Min                            -0.231763
evaluation/Returns Mean                           -0.243855
evaluation/Returns Std                             0.511237
evaluation/Returns Max                             0.337444
evaluation/Returns Min                            -0.948814
evaluation/Actions Mean                            5.81711e-06
evaluation/Actions Std                             2.41509e-05
evaluation/Actions Max                             0.000927147
evaluation/Actions Min                            -0.000939695
evaluation/Num Paths                               5
evaluation/Average Returns                        -0.243855
evaluation/env_infos/final/reward_run Mean        -1.38778e-17
evaluation/env_infos/final/reward_run Std          1.54537e-16
evaluation/env_infos/final/reward_run Max          2.08167e-16
evaluation/env_infos/final/reward_run Min         -2.77556e-16
evaluation/env_infos/initial/reward_run Mean      -0.0181035
evaluation/env_infos/initial/reward_run Std        0.0729257
evaluation/env_infos/initial/reward_run Max        0.0444043
evaluation/env_infos/initial/reward_run Min       -0.144429
evaluation/env_infos/reward_run Mean              -0.000243855
evaluation/env_infos/reward_run Std                0.0126893
evaluation/env_infos/reward_run Max                0.200411
evaluation/env_infos/reward_run Min               -0.231763
evaluation/env_infos/final/reward_ctrl Mean       -1.50766e-10
evaluation/env_infos/final/reward_ctrl Std         0
evaluation/env_infos/final/reward_ctrl Max        -1.50766e-10
evaluation/env_infos/final/reward_ctrl Min        -1.50766e-10
evaluation/env_infos/initial/reward_ctrl Mean     -4.64648e-10
evaluation/env_infos/initial/reward_ctrl Std       1.52297e-10
evaluation/env_infos/initial/reward_ctrl Max      -2.22531e-10
evaluation/env_infos/initial/reward_ctrl Min      -6.55298e-10
evaluation/env_infos/reward_ctrl Mean             -3.70086e-10
evaluation/env_infos/reward_ctrl Std               4.17319e-09
evaluation/env_infos/reward_ctrl Max              -6.80181e-11
evaluation/env_infos/reward_ctrl Min              -2.19241e-07
time/data storing (s)                              0.0068261
time/evaluation sampling (s)                       2.83655
time/exploration sampling (s)                      0.643244
time/logging (s)                                   0.0246991
time/sac training (s)                             74.7078
time/saving (s)                                    0.0204751
time/training (s)                                  0.000113903
time/epoch (s)                                    78.2397
time/total (s)                                    79.4814
Epoch                                              0
----------------------------------------------  --------------
